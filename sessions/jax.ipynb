{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d157ba1-2628-4b43-a768-e8818af6de19",
   "metadata": {},
   "source": [
    "![Py4Eng](img/logo.png)\n",
    "\n",
    "# JAX: automatic differentiation, just-in-time compilation, and acceleration\n",
    "## Yoav Ram\n",
    "\n",
    "[JAX](http://jax.readthedocs.io/) is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning.\n",
    "\n",
    "JAX combines automatic differentiation and a machine-learning specific compiler ([XLA](https://www.tensorflow.org/xla)) for high-performance numerical computing.\n",
    "\n",
    "Benefits of JAX:\n",
    "- a familiar NumPy-style API for ease of adoption,\n",
    "- composable function transformations for **compilation**, **batching/vectorization**, **automatic differentiation**, and **parallelization**,\n",
    "- The same code executes on multiple backend accelerators, including CPU, GPU, and TPU (Google's GPU)\n",
    "\n",
    "JAX allows us to just-in-time compile functions and importantly to compute gradients automatically. We will see these features as we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2b7892-0bf1-45a5-a39a-94cc1a2a3159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX 0.5.2 on cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "print('JAX', jax.__version__, \"on\", jax.default_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0b908-c849-4736-a7c4-10c81732d2f5",
   "metadata": {},
   "source": [
    "# Example: Learning XOR with neural network\n",
    "\n",
    "As an example, we will build a neural network that learns the Exclusive OR (XOR) function, following a [tutorial by Collin Raffel](https://colinraffel.com/blog/you-don-t-know-jax.html).\n",
    "\n",
    "Reminder: XOR takes two binary inputs and returns a single binary output so that the output is 0 if the inputs are equal (1 and 1 or 0 and 0) and 1 if the inputs are different (0 and 1 or 1 and 0).\n",
    "\n",
    "We'll use a small neural network so that for the input $x$\n",
    "$$\n",
    "z = \\tanh(w_1 x + b_1) $$$$\n",
    "y = \\sigma(w_2 z + b_2)\n",
    "$$\n",
    "where the output is $y$ and the hidden layer is $z$. \n",
    "The activation functions are the hyperbolic tangent `tanh` in the first layer, and the expit/sigmoid/logistic $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "The size of $x$ is 2, the size of $y$ is 1, and we set the size of $z$ to be 3.\n",
    "Therefore, the parameters are $w_1$ of size 2x3, $b_1$ of size 3 , $w_2$ of size 3x1, and $b_2$ of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274cc36a-89f1-4a5c-9d75-d585284fb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def nn(params, x):\n",
    "    w1, b1, w2, b2 = params\n",
    "    z = np.tanh(x @ w1 + b1)\n",
    "    return sigmoid(z @ w2 + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f1e42-c080-4204-a074-8ae5d123d396",
   "metadata": {},
   "source": [
    "We need to initilizae the parameters from a random distribution. \n",
    "\n",
    "JAX uses a stateless pseudorandom number generator (PRNG), because JAX is stateless to support parallelizaiton, compilation, and differentiation. Therefore, we have to keep track of the state of the PRNG, which is called **key**. We do so by producing a new key from an integer of our choice with `jax.random.key(k)`, and by splitting an existing key to `n` keys with `jax.random.split(k, n)`. The PRNG can then consume a key to create arrays of random numbers.\n",
    "\n",
    "Note: Given a value of `k`, the sequence of random numbers is determined and replicable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76378b0-a59a-4a1b-b593-479a5fb0b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(key):\n",
    "    subkeys = jax.random.split(key, 4)\n",
    "    w1 = jax.random.normal(subkeys[0], shape=(2, 3))\n",
    "    b1 = jax.random.normal(subkeys[1], shape=(3,))\n",
    "    w2 = jax.random.normal(subkeys[2], shape=(3,))\n",
    "    b2 = jax.random.normal(subkeys[3], shape=(1,))\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "key = jax.random.key(9)\n",
    "subkey, key = jax.random.split(key)\n",
    "params = init_params(subkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe08bc-cbc6-433a-a6f7-ce03ef2caa7e",
   "metadata": {},
   "source": [
    "We train with the neural network by minimizing the cross entropy loss function (i.e., the negative log likelihood of the Bernoulli distribution) via stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c043b8-f63c-43cd-ae3e-5b88ebf8dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, X, Y, ϵ=1e-10):\n",
    "    Yhat = nn(params, X)\n",
    "    return -(Y * np.log(Yhat + ϵ) + (1 - Y) * np.log(1 - Yhat + ϵ)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f9e81-d404-4a9a-bc22-4b0a46524205",
   "metadata": {},
   "source": [
    "The gradient of the loss function with respect to the network parameters can be automatically differentiated with `jax.grad`, which takes a function `f` and returns a new function `df` which computes the gradient of the original function with respect to a specific argument (be default, the first argument).\n",
    "\n",
    "To use gradient descent, we want to be able to compute the gradient of our loss function with respect to our neural network's parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09c8f09-3bb2-4775-a879-13fe166c1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = jax.grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd89a12-fef8-4c59-9fca-d775e827053e",
   "metadata": {},
   "source": [
    "That was easy!\n",
    "\n",
    "Now we implement the gradient descent algorithm for minimization of the loss function, in which we update the parameters using the gradient descent rule,\n",
    "$$\n",
    "w = w - \\eta \\cdot \\frac{d L}{d w}(w, X, Y)\n",
    "$$\n",
    "where $w$ is a parameter, $\\eta$ is called _learning rate_, and $L$ is the loss function.\n",
    "\n",
    "Usually, gradient descent is repeated until some threshold is met or for a set amount of iterations. Here, we will continue until the network learned all four combinations.\n",
    "So here we write a `test` function that checks if the network has learned everything we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4825ec4d-18e3-4551-8b84-41028bb8ba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(False, dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the data\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.bitwise_xor(X[:,0], X[:, 1])\n",
    "\n",
    "# test function\n",
    "def test(params, X, Y):\n",
    "    Yhat = (nn(params, X) > 0.5).astype(int)\n",
    "    return (Yhat == Y).all()\n",
    "\n",
    "test(params, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a89a8-22bd-41b6-b758-669430a8626d",
   "metadata": {},
   "source": [
    "Let the training begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640486e4-6cc5-47c2-abdf-ce59cda0bef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7860451\n",
      "Test: False\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(9)\n",
    "subkey, key = jax.random.split(key)\n",
    "params = init_params(subkey)\n",
    "print(\"Loss:\", loss(params, X, Y))\n",
    "print(\"Test:\", test(params, X, Y))\n",
    "η = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52384819-f407-4cb3-889a-8df322d191a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.69145\n",
      "20: 0.68096\n",
      "30: 0.65436\n",
      "40: 0.60349\n",
      "50: 0.54253\n",
      "60: 0.49203\n",
      "70: 0.45673\n",
      "80: 0.43273\n",
      "90: 0.41603\n",
      "100: 0.40400\n",
      "110: 0.39503\n",
      "120: 0.38810\n",
      "130: 0.38259\n",
      "140: 0.37808\n",
      "150: 0.37426\n",
      "160: 0.37089\n",
      "170: 0.36778\n",
      "180: 0.36465\n",
      "190: 0.36108\n",
      "200: 0.35601\n",
      "210: 0.34557\n",
      "CPU times: user 1.58 s, sys: 49.2 ms, total: 1.63 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = 0\n",
    "while True:\n",
    "    t += 1\n",
    "    grads = loss_grad(params, X, Y)\n",
    "    params = [\n",
    "        p - η * g \n",
    "        for p, g in zip(params, grads)\n",
    "    ]\n",
    "    if t % 10 == 0:\n",
    "        print(\"{:d}: {:.5f}\".format(t, loss(params, X, Y)))\n",
    "        if test(params, X, Y):\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bbfefe9-30df-4f24-8590-cd0b05a8df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print((nn(params, X) > 0.5).astype(int))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac9b50-fc08-4a0f-b9a9-c7994e638aa0",
   "metadata": {},
   "source": [
    "So we have trained a neural network on the XOR function succesfully, and rather fast. But we can make it even faster if we jit compile the computational intensive part. First, we refactor the code so that the gradient computation and the parameter updating is in a single separate function, and then we compile that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb1c5ff2-362e-4309-9933-0803af9586be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.06 ms ± 160 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "12.6 μs ± 189 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(params, X, Y):\n",
    "    grads = loss_grad(params, X, Y)\n",
    "    params = [\n",
    "        p - η * g \n",
    "        for p, g in zip(params, grads)\n",
    "    ]\n",
    "    return params\n",
    "\n",
    "%timeit gradient_descent(params, X, Y)\n",
    "gradient_descent = jax.jit(gradient_descent)\n",
    "gradient_descent(params, X, Y) # burn-in\n",
    "%timeit gradient_descent(params, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992abff-a4d7-4e06-87bd-e514afd7269e",
   "metadata": {},
   "source": [
    "That's a significant improvement of about 400-fold.\n",
    "\n",
    "Now run the training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60b0bd99-9b93-42c9-93ce-ecc255720833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7860451\n",
      "Test: False\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(9)\n",
    "subkey, key = jax.random.split(key)\n",
    "params = init_params(subkey)\n",
    "print(\"Loss:\", loss(params, X, Y))\n",
    "print(\"Test:\", test(params, X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8266d8fa-7524-4ac8-b4b9-536992b3fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.69145\n",
      "20: 0.68096\n",
      "30: 0.65436\n",
      "40: 0.60349\n",
      "50: 0.54253\n",
      "60: 0.49203\n",
      "70: 0.45673\n",
      "80: 0.43273\n",
      "90: 0.41603\n",
      "100: 0.40400\n",
      "110: 0.39503\n",
      "120: 0.38810\n",
      "130: 0.38259\n",
      "140: 0.37808\n",
      "150: 0.37426\n",
      "160: 0.37089\n",
      "170: 0.36778\n",
      "180: 0.36465\n",
      "190: 0.36108\n",
      "200: 0.35601\n",
      "210: 0.34557\n",
      "CPU times: user 134 ms, sys: 3.51 ms, total: 138 ms\n",
      "Wall time: 76.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "η = 1.0\n",
    "t = 0\n",
    "while True:\n",
    "    t += 1\n",
    "    params = gradient_descent(params, X, Y)\n",
    "    if t % 10 == 0:\n",
    "        print(\"{:d}: {:.5f}\".format(t, loss(params, X, Y)))\n",
    "        if test(params, X, Y):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "da847959-3b2b-41b3-bf1c-7c872d0e34d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print((nn(params, X) > 0.5).astype(int))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2590c-7de0-475d-9926-a4700f2c24d3",
   "metadata": {},
   "source": [
    "So overall training time was reduced from 1.2 s to 76 ms, a 15-fold improvement. With larger datasets and neural networks, we can use a multicore CPU, GPU, or TPU with JAX without modifying the code to get even better improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a06bd2-27c2-4f1c-9ab8-f7c919a4f9c3",
   "metadata": {},
   "source": [
    "# Differences from standard NumPy\n",
    "\n",
    "When using JAX we can mostly use the NumPy API, with specific differences:\n",
    "- JAX arrays are immutable so we cannot use item assignment (`arr[i] = x` is not allowed)\n",
    "- random number generation requires us to provide a random key at every call because the random number generator is stateless.\n",
    "- jit-compiling functions with branching conrol flow (`if` etc.) is problematic, as a single run of the function probably doesn't go through all the relevant paths; this can be solved by using `jax.lax.cond` etc. (see more [in the docs](https://docs.jax.dev/en/latest/control-flow.html#control-flow)).\n",
    "\n",
    "A more comprehensive list is available at the JAX docs under [\"The Sharp Bits\"](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n",
    "\n",
    "## Changing an array\n",
    "\n",
    "Since JAX arrays are immutable, we cannot set to an array as we would in NumPy, that is, we cannot do\n",
    "```python\n",
    "arr[i] = 5\n",
    "```\n",
    "Instead, we can use a JAX equivalent\n",
    "```python\n",
    "arr = arr.at[i].set(5)\n",
    "```\n",
    "This creates a new array with 5 at index `i` and sets the new array to the variable name `arr`.\n",
    "However, during compilation, the compiler can decide to apply the item assignment in place instead of creating a new array.\n",
    "\n",
    "Here's an example in a function that generates the first 1000 Bernoulli numbers, defined by\n",
    "$$\n",
    "a_i = a_{i-1} + a_{i-2}, \\quad\n",
    "a_0 = 1, \\quad\n",
    "a_1 = 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "310f0b5c-be96-4402-853a-c92d1fdee38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 ms ± 1.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def bernoulli():\n",
    "    n = 1000\n",
    "    a = np.zeros(n)\n",
    "    a = a.at[0].set(1)\n",
    "    a = a.at[1].set(2)\n",
    "    for i in range(2, n):\n",
    "        a = a.at[i].set(a[i-1] + a[i-2])\n",
    "    return a\n",
    "\n",
    "%timeit bernoulli();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbcf80-71de-4105-a0f5-598d27c48832",
   "metadata": {},
   "source": [
    "Compiling this, we can a huge improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fed2c8ce-700a-42e4-b3e6-8195cea74b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4 μs ± 77.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "bernoulli = jax.jit(bernoulli)\n",
    "bernoulli();\n",
    "%timeit bernoulli();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb8b83-5914-4c81-b1c6-db917b628c74",
   "metadata": {},
   "source": [
    "## Static arguments\n",
    "\n",
    "If we want the `bernoulli` function to take the value of `n` from the user, we cannot compile it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "29604d18-1611-42d4-bcb3-ed54b0f66c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function bernoulli at /var/folders/vc/qm7741c57dsg9f7wyrtrdrrm0000gq/T/ipykernel_60395/1599531239.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument n.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m         a = a.at[i].set(a[i-\u001b[32m1\u001b[39m] + a[i-\u001b[32m2\u001b[39m])\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mbernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mbernoulli\u001b[39m\u001b[34m(n)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@jax\u001b[39m.jit\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbernoulli\u001b[39m(n):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     a = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     a = a.at[\u001b[32m0\u001b[39m].set(\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m     a = a.at[\u001b[32m1\u001b[39m].set(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/HPCPy/lib/python3.13/site-packages/jax/_src/numpy/array_creation.py:77\u001b[39m, in \u001b[36mzeros\u001b[39m\u001b[34m(shape, dtype, device)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (m := _check_forgot_shape_tuple(\u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m, shape, dtype)): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(m)\n\u001b[32m     76\u001b[39m dtypes.check_user_dtype_supported(dtype, \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m shape = \u001b[43mcanonicalize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lax.full(shape, \u001b[32m0\u001b[39m, dtypes.jax_dtype(dtype), sharding=util.normalize_device_to_sharding(device))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/HPCPy/lib/python3.13/site-packages/jax/_src/numpy/array_creation.py:39\u001b[39m, in \u001b[36mcanonicalize_shape\u001b[39m\u001b[34m(shape, context)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcanonicalize_shape\u001b[39m(shape: Any, context: \u001b[38;5;28mstr\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) -> core.Shape:\n\u001b[32m     37\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m     38\u001b[39m       (\u001b[38;5;28mgetattr\u001b[39m(shape, \u001b[33m'\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m np.ndim(shape) == \u001b[32m0\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanonicalize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core.canonicalize_shape(shape, context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/HPCPy/lib/python3.13/site-packages/jax/_src/core.py:1717\u001b[39m, in \u001b[36mcanonicalize_shape\u001b[39m\u001b[34m(shape, context)\u001b[39m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1716\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1717\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[31mTypeError\u001b[39m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function bernoulli at /var/folders/vc/qm7741c57dsg9f7wyrtrdrrm0000gq/T/ipykernel_60395/1599531239.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument n."
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def bernoulli(n):\n",
    "    a = np.zeros(n)\n",
    "    a = a.at[0].set(1)\n",
    "    a = a.at[1].set(2)\n",
    "    for i in range(2, n):\n",
    "        a = a.at[i].set(a[i-1] + a[i-2])\n",
    "    return a\n",
    "\n",
    "bernoulli(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cd310-98e8-45e5-9481-07a8008d02da",
   "metadata": {},
   "source": [
    "This occurs because the shape of an array (in `np.zeros(n)`) cannot be set to a traced value, which is what the arguments of a function are, by default. So we can set `n` to be a static argument. This means, however, that when the function is compiled, it is compiled for a specific *value* of `n` rather than a specific *type* of `n`. So in this case, it defeats the purpose, as we are not going to call the function on the same value of `n` more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c2c3967-7f8a-48f5-a13f-bab8d8e9b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli(n):\n",
    "    a = np.zeros(n)\n",
    "    a = a.at[0].set(1)\n",
    "    a = a.at[1].set(2)\n",
    "    for i in range(2, n):\n",
    "        a = a.at[i].set(a[i-1] + a[i-2])\n",
    "    return a\n",
    "\n",
    "bernoulli = jax.jit(bernoulli, static_argnames='n')\n",
    "bernoulli(1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae2ba462-61ed-484d-8813-0c91e5852222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.47 μs ± 27.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bernoulli(1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09953cd-bce5-4814-9cec-dc94c6525800",
   "metadata": {},
   "source": [
    "# Mapping\n",
    "\n",
    "JAX allows us to vectorize a function by mapping it over input dimensions.\n",
    "\n",
    "Consider the following function that computes the Hamming distance between two 1D arrays (i.e., the number of elements in which the two arrays differ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b96ed0fe-22e2-4c8a-aa8f-007333a7cae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4, dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamming1D(x, y):\n",
    "    return (x!=y).sum()\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([5, 4, 3, 2, 1])\n",
    "hamming1D(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4ebb0-060d-412d-b367-42eba224944a",
   "metadata": {},
   "source": [
    "What if we want to compute the distance between a single array `x` and all the rows of a matrix `Y`? We would need a for loop over the rows.\n",
    "\n",
    "With JAX, we can use `jax.vmap`. We use the argument `in_axes` to specify the axes/dimensions of each argument on which we want to map. Here, we don't want to map over `x`, so we put `None`, and we want to map over the 1st dimension of `Y`, so we put `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8535a6e5-c38f-46a2-b20c-f26e81bd491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5, 5, 4, 4, 5, 5, 5, 4, 5, 2], dtype=int32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = jax.random.randint(jax.random.key(4), shape=(10, 5), minval=1, maxval=5)\n",
    "hamming2D = jax.jit(jax.vmap(hamming1D, in_axes=[None, 0]))\n",
    "hamming2D(x, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77629ecd-38dd-416a-ab2a-d96480ed9e63",
   "metadata": {},
   "source": [
    "# References\n",
    "- [You don't know JAX by Colin Raffel](https://colinraffel.com/blog/you-don-t-know-jax.html)\n",
    "- [Comparison of NumPy, Numba, JAX, and C on Mandelbrot's fractal](https://gist.github.com/jpivarski/da343abd8024834ee8c5aaba691aafc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4ba44-4087-451d-be1b-f961462292a8",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com).\n",
    "\n",
    "This work is licensed under a CC BY-NC-SA 4.0 International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HPCPy]",
   "language": "python",
   "name": "conda-env-HPCPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
